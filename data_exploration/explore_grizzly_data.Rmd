---
title: "Exploring Grizzly Data"
author: ""
date: "June 5, 2015"
output:
  html_document:
    keep_md: true
---

In this lesson, we are going to be using open data from [DataBC](http://data.gov.bc.ca) to learn about cleaning and tidying data, and merging tabular data with spatial data so we can visualize it.

We're going to use a number of packages in this lesson, so we will start with loading them all:

```{r message=FALSE}
library(tidyr)
library(dplyr)
library(knitr)
library(sp)
library(rgdal)
library(ggplot2)

options(knitr.table.format = "markdown")
```

### Grizzly bear data: population estimates and mortality history

DataBC has data on [population estimates](http://catalogue.data.gov.bc.ca/dataset/2012-grizzly-bear-population-estimates) and [mortality](http://catalogue.data.gov.bc.ca/dataset/history-of-grizzly-bear-mortalities). Let's download and explore both:

```{r}
## getting the data
mortality <- read.csv("http://www.env.gov.bc.ca/soe/archive/data/plants-and-animals/2012_Grizzly_Status/Grizzly_bear_mortality_history.csv", stringsAsFactors = FALSE)

population <- read.csv("http://www.env.gov.bc.ca/soe/archive/data/plants-and-animals/2012_Grizzly_Status/Grizzly_population_estimate_2012.csv", stringsAsFactors = FALSE)
```

Now that we have the data, let's have a look. The `head` function shows us the first six rows of data: 

```{r results='asis'}
kable(head(mortality))
```

We're going to use the [dplyr](https://github.com/hadley/dplyr) and [tidyr](https://github.com/hadley/tidyr) packages to organize and clean our data. First, we'll work on the mortality data:

```{r results='asis'}

## let's get rid of unused columns
mortality <- mortality %>% 
  select(-contains("X."))

# Now we can separate the AGE_CLASS column into two columns specifying the 
# minumum age and the maximum age
clean_mort <- mortality %>%
  separate(AGE_CLASS, into = c("minimum_age", "maximum_age"), sep = "-", 
           extra = "merge") %>% 
  mutate(minimum_age = extract_numeric(minimum_age),
         maximum_age = extract_numeric(maximum_age))

kable(head(clean_mort))
```

Next let's tidy up the population data. The first, eighth, and ninth columns (`X.`) doesn't contain any useful information, so we can get rid if it:

```{r}
## We can do it directly on each column like so:
# population$X <- NULL
# population$X.1 <- NULL
# population$X.2 <- NULL

## Or we can use a loop to do it.  This would be effective if there 
## were a lot of columns we wanted to get rid of

for (n in names(population)) {
  if (grepl("X", n)) {
    population[n] <- NULL
  }
}
```

Now we see that there is some metadata in the first five rows of the `Notes.` column. We should remove it and store it in a variable:

```{r}
population_meta <- paste(population$Notes.[1:5], collapse = "; ")
population$Notes. <- NULL
```

We can store the metadata as a comment attribute of the population data frame:

```{r}
comment(population) <- population_meta
```

We can view the comment using similar syntax:

```{r}
comment(population)
```

The population estimates are split up by Population Unit (`GBPU`) and Management Unit (`MU`). Let's summarise by Population Unit. `dplyr` has some great functions for this sort of exploratory analysis.

```{r}
## First we set the grouping variable to be GBPU
population_gbpu <- group_by(population, GBPU)

## Then we can summarize based on those groups. We will need to recalculate 
## the density.
population_gbpu <- summarise(population_gbpu, 
                             Estimate = sum(Estimate, na.rm = TRUE), 
                             Total_Area = sum(Total_Area, na.rm = TRUE), 
                             Density = Estimate / Total_Area * 1000)
head(population_gbpu)
```

### Grizzly Bear Population Units spatial data

First, you will need to get the data. Unfortunately, we are unable to read/download this directly from DataBC. Visit the [metadata record](http://catalogue.data.gov.bc.ca/dataset/grizzly-bear-population-units/resource/7a7713f9-bcbd-46b8-968a-03d343d367fb) for the data at DataBC. Click on the **Go To Resource** button and submit the form with the following settings:

![](img/Griz_form.png)

When you get the email with the link to the zip file, save it in your working directory as `data/DataBC_GBPU.zip`

Next we'll unzip the file, and import the shapefile. You will need the `sp` and `rgdal` packages: 

- The `sp` package provides classes (data structures) for spatial objects, including `SpatialPoints`, 
`SpatialPolygons`, `SpatialLines`, etc. Each of these classes has a complementary class that can hold attributes that go along with the spatial data. They are `SpatialPointsDataFrame`, `SpatialPolygonsDataFrame`, `SpatialLinesDataFrame`. You always need this package when working with spatial data.

- The `rgdal` package provides input/output and projection/transformation operations for spatial data via bindings to the Geospatial Data Abstraction Library (GDAL).

```{r messate = FALSE}
library(sp)
library(rgdal)
zipfile <- "data/DataBC_GBPU.zip"

if (!file.exists(zipfile)) {
  download.file("https://github.com/ateucher/BCenviroLessons/raw/master/data_exploration/data/DataBC_GBPU.zip", destfile = zipfile)
}

unzip(zipfile, exdir = "data")

gbpu <- readOGR(dsn = "data/GBPU_BC", layer = "GBPU_BC_polygon", 
                encoding = "ESRI Shapefile", stringsAsFactors = FALSE)
```

Let's explore the gbpu spatial object. It is of class `SpatialPolygonsDataFrame`, which is a special class of **R** object for representing spatial data, implemented in the `sp` package.

```{r}
class(gbpu)
summary(gbpu)
```

`sp` objects are S4 objects - which are kind of like regular S3 lists, but are more formally defined. They also have a special type of field, called slots, that are accessed using the `@` operator. You use this in the same way you use the `$` to access list elements, or columns in a data frame.

We can see the slotNames in an `sp` object using the `slotNames()` funcion, and look at any of them using `@`:

```{r}
slotNames(gbpu)
head(gbpu@data)
```


From the [metadata page](http://catalogue.data.gov.bc.ca/dataset/grizzly-bear-population-units/resource/7a7713f9-bcbd-46b8-968a-03d343d367fb), we know that there are several versions of the population units in this file. From the summary above it looks like the version is stored in the `GBPU_VERS`. Let's only use the latest version (2012). Note that we can use subsetting using `$` and `[` just like we do on normal data frames.

We can then plot the polygons to have a quick look.

```{r gbpu_map}
gbpu_2012 <- gbpu[gbpu$GBPU_VERS == 2012, ]
plot(gbpu_2012)
```

Now that we have a map of GBPUs, and a data frame with a single population estimate per GBPU, we can merge the population estimates into the SpatialPolygonsDataFrame.

```{r}
## First we should make sure that the GBPU names in the two objects are the same
setdiff(na.omit(gbpu_2012$GBPU_NAME), population_gbpu$GBPU)
setdiff(population_gbpu$GBPU, na.omit(gbpu_2012$GBPU_NAME))
population_gbpu$GBPU[population_gbpu$GBPU == "Central Purcells"] <- "Central-South Purcells"
population_gbpu$GBPU[population_gbpu$GBPU == "North Purcell"] <- "North Purcells"

## Check to make sure they are equal now
all.equal(sort(population_gbpu$GBPU), sort(na.omit(gbpu_2012$GBPU_NAME)))
```

Now lets plot the map using ggplot2, and fill the polygons based on population density. First we need to use the function `fortify` from `ggplot2`, to turn the SpatialPolygonsDataFrame into a data frame, then merge the attributes from `population_gbpu`:

```{r density-map}
gbpu_df <- fortify(gbpu_2012, region = "GBPU_NAME")
gbpu_df <- merge(gbpu_df, population_gbpu, by.x = "id", by.y = "GBPU")

ggplot(gbpu_df, aes(x = long, y = lat, group = group)) + 
  geom_polygon(aes(fill = Density)) + 
  geom_path(colour = "white") + 
  coord_map() + 
  theme_minimal()
```

### Getting polygon attributes into point data

Let's create a random assortment of points we collected in BC:

```{r points}
pts <- data.frame(id = 1:100, 
                  x = runif(100, gbpu@bbox["x", "min"], gbpu@bbox["x", "max"]), 
                  y = runif(100, gbpu@bbox["y", "min"], gbpu@bbox["y", "max"]))

```

To convert a data frame of lats and longs to a SpatialPointsDataFrame, use the `coordinates()` function from the `sp` package

```{r}
coordinates(pts) <- ~x+y
proj4string(pts)
```

We know that the points were collected in the same coordinate system as the gbpu data (lat/long, unprojected)

```{r}
proj4string(pts) <- proj4string(gbpu_2012)
pt_attributes <- over(pts, gbpu_2012)
pts$GBPU_NAME <- pt_attributes$GBPU_NAME
head(pts@data)
```

